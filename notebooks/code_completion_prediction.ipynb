{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mt3uMxC0GS1n",
    "outputId": "9264fb59-bd38-410f-96ad-2067fa106282"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Obtaining dependency information for torch from https://files.pythonhosted.org/packages/57/6c/bf52ff061da33deb9f94f4121fde7ff3058812cb7d2036c97bc167793bd1/torch-2.5.1-cp312-none-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading torch-2.5.1-cp312-none-macosx_11_0_arm64.whl.metadata (28 kB)\n",
      "Collecting torchvision\n",
      "  Obtaining dependency information for torchvision from https://files.pythonhosted.org/packages/c5/eb/4ba19616378f2bc085999432fded2b7dfdbdccc6dd0fc293203452508100/torchvision-0.20.1-cp312-cp312-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading torchvision-0.20.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.1 kB)\n",
      "Collecting torchaudio\n",
      "  Obtaining dependency information for torchaudio from https://files.pythonhosted.org/packages/03/ab/151037a41e2cf4a5d489dfe5e7196b755e0fd83958d5ca7ad8ed85afcb1c/torchaudio-2.5.1-cp312-cp312-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading torchaudio-2.5.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.4 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Obtaining dependency information for filelock from https://files.pythonhosted.org/packages/b9/f8/feced7779d755758a52d1f6635d990b8d98dc0a29fa568bbe0625f18fdf3/filelock-3.16.1-py3-none-any.whl.metadata\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/yash_bharatiya/Desktop/ASSIGNMENTS/SSD/ssl-code-complete/venv/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Collecting networkx (from torch)\n",
      "  Obtaining dependency information for networkx from https://files.pythonhosted.org/packages/b9/54/dd730b32ea14ea797530a4479b2ed46a6fb250f682a9cfb997e968bf0261/networkx-3.4.2-py3-none-any.whl.metadata\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in /Users/yash_bharatiya/Desktop/ASSIGNMENTS/SSD/ssl-code-complete/venv/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Collecting fsspec (from torch)\n",
      "  Obtaining dependency information for fsspec from https://files.pythonhosted.org/packages/c6/b2/454d6e7f0158951d8a78c2e1eb4f69ae81beb8dca5fee9809c6c99e9d0d0/fsspec-2024.10.0-py3-none-any.whl.metadata\n",
      "  Downloading fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: setuptools in /Users/yash_bharatiya/Desktop/ASSIGNMENTS/SSD/ssl-code-complete/venv/lib/python3.12/site-packages (from torch) (75.6.0)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Obtaining dependency information for sympy==1.13.1 from https://files.pythonhosted.org/packages/b2/fe/81695a1aa331a842b582453b605175f419fe8540355886031328089d840a/sympy-1.13.1-py3-none-any.whl.metadata\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Obtaining dependency information for mpmath<1.4,>=1.1.0 from https://files.pythonhosted.org/packages/43/e3/7d92a15f894aa0c9c4b49b8ee9ac9850d6e63b03c9c32c0367a13ae62209/mpmath-1.3.0-py3-none-any.whl.metadata\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: numpy in /Users/yash_bharatiya/Desktop/ASSIGNMENTS/SSD/ssl-code-complete/venv/lib/python3.12/site-packages (from torchvision) (1.26.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/yash_bharatiya/Desktop/ASSIGNMENTS/SSD/ssl-code-complete/venv/lib/python3.12/site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/yash_bharatiya/Desktop/ASSIGNMENTS/SSD/ssl-code-complete/venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading torch-2.5.1-cp312-none-macosx_11_0_arm64.whl (63.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.20.1-cp312-cp312-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchaudio-2.5.1-cp312-cp312-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.6/179.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mpmath, sympy, networkx, fsspec, filelock, torch, torchvision, torchaudio\n",
      "Successfully installed filelock-3.16.1 fsspec-2024.10.0 mpmath-1.3.0 networkx-3.4.2 sympy-1.13.1 torch-2.5.1 torchaudio-2.5.1 torchvision-0.20.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting gdown\n",
      "  Obtaining dependency information for gdown from https://files.pythonhosted.org/packages/54/70/e07c381e6488a77094f04c85c9caf1c8008cdc30778f7019bc52e5285ef0/gdown-5.2.0-py3-none-any.whl.metadata\n",
      "  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/yash_bharatiya/Desktop/ASSIGNMENTS/SSD/ssl-code-complete/venv/lib/python3.12/site-packages (from gdown) (4.12.3)\n",
      "Requirement already satisfied: filelock in /Users/yash_bharatiya/Desktop/ASSIGNMENTS/SSD/ssl-code-complete/venv/lib/python3.12/site-packages (from gdown) (3.16.1)\n",
      "Requirement already satisfied: requests[socks] in /Users/yash_bharatiya/Desktop/ASSIGNMENTS/SSD/ssl-code-complete/venv/lib/python3.12/site-packages (from gdown) (2.32.3)\n",
      "Collecting tqdm (from gdown)\n",
      "  Obtaining dependency information for tqdm from https://files.pythonhosted.org/packages/d0/30/dc54f88dd4a2b5dc8a0279bdd7270e735851848b762aeb1c1184ed1f6b14/tqdm-4.67.1-py3-none-any.whl.metadata\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /Users/yash_bharatiya/Desktop/ASSIGNMENTS/SSD/ssl-code-complete/venv/lib/python3.12/site-packages (from beautifulsoup4->gdown) (2.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/yash_bharatiya/Desktop/ASSIGNMENTS/SSD/ssl-code-complete/venv/lib/python3.12/site-packages (from requests[socks]->gdown) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/yash_bharatiya/Desktop/ASSIGNMENTS/SSD/ssl-code-complete/venv/lib/python3.12/site-packages (from requests[socks]->gdown) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/yash_bharatiya/Desktop/ASSIGNMENTS/SSD/ssl-code-complete/venv/lib/python3.12/site-packages (from requests[socks]->gdown) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/yash_bharatiya/Desktop/ASSIGNMENTS/SSD/ssl-code-complete/venv/lib/python3.12/site-packages (from requests[socks]->gdown) (2024.8.30)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6 (from requests[socks]->gdown)\n",
      "  Obtaining dependency information for PySocks!=1.5.7,>=1.5.6 from https://files.pythonhosted.org/packages/8d/59/b4572118e098ac8e46e399a1dd0f2d85403ce8bbaad9ec79373ed6badaf9/PySocks-1.7.1-py3-none-any.whl.metadata\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: tqdm, PySocks, gdown\n",
      "Successfully installed PySocks-1.7.1 gdown-5.2.0 tqdm-4.67.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: numpy in /Users/yash_bharatiya/Desktop/ASSIGNMENTS/SSD/ssl-code-complete/venv/lib/python3.12/site-packages (1.26.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: tqdm in /Users/yash_bharatiya/Desktop/ASSIGNMENTS/SSD/ssl-code-complete/venv/lib/python3.12/site-packages (4.67.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio\n",
    "!pip install gdown\n",
    "!pip install numpy\n",
    "!pip install tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tlS-dXgPGVWk",
    "outputId": "1567a2fa-694a-4cbe-a095-709b68488da7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: pip\n",
      "zsh:1: no matches found: https://drive.google.com/uc?id=0B2i-vWnOu7MxVlJwQXN6eVNONUU\n",
      "tar: Error opening archive: Failed to open 'programs.tar.gz'\n"
     ]
    }
   ],
   "source": [
    "!mkdir dataset\n",
    "!cd dataset\n",
    "!pip install gdown\n",
    "!gdown https://drive.google.com/uc?id=0B2i-vWnOu7MxVlJwQXN6eVNONUU\n",
    "!tar -xvf programs.tar.gz\n",
    "!cd ..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D6JZay3JGW1B",
    "outputId": "3d59b7a8-c033-4d33-f9ce-291eda541e8e"
   },
   "outputs": [],
   "source": [
    "!python preprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NvyKbZumGcLx",
    "outputId": "0da1da7a-beda-485b-df61-fef44bacd0c3"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow numpy pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 463
    },
    "id": "ZfmfE-UvG_hh",
    "outputId": "9f2af581-8e59-4865-bc40-03eaab6706f4"
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudiol torch torchvision torchaudio\n",
    "!pip install gdown\n",
    "!pip install numpy\n",
    "!pip install tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "id": "EahG9e3wlQ4I",
    "outputId": "439474db-9650-404d-a922-2a240e19846b"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "import re\n",
    "\n",
    "# Load the data from train.json\n",
    "def load_data(file_path, sample_fraction=0.01):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    # Sample 1% of the data\n",
    "    sample_size = max(1, int(len(data) * sample_fraction))\n",
    "    return [item['code'] for item in data[:sample_size]]\n",
    "\n",
    "# Custom tokenization function to include symbols and ignore comments\n",
    "def custom_tokenize(code):\n",
    "    # Remove comments that start with //\n",
    "    code = re.sub(r'//.*', '', code)  # Remove everything after //\n",
    "    tokens = re.findall(r'\\w+|[^\\w\\s]', code)\n",
    "    return tokens\n",
    "\n",
    "# Prepare the dataset\n",
    "codes = load_data('train.jsonl')\n",
    "data = ' '.join(codes)\n",
    "\n",
    "# Tokenization\n",
    "input_sequences = []\n",
    "for line in data.split('\\n'):\n",
    "    # Use custom tokenizer instead of the default tokenizer\n",
    "    token_list = custom_tokenize(line)\n",
    "\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i + 1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "# Create the tokenizer and fit on the tokenized sequences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([' '.join(seq) for seq in input_sequences])  # Fit on space-joined sequences\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "input_sequences = [tokenizer.texts_to_sequences([' '.join(seq)])[0] for seq in input_sequences]\n",
    "\n",
    "\n",
    "# Pad sequences\n",
    "max_sequence_length = max(len(x) for x in input_sequences)\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre')\n",
    "\n",
    "# Create predictors and labels\n",
    "X, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n",
    "\n",
    "# Step 2: Model Creation\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length=max_sequence_length - 1))\n",
    "model.add(SimpleRNN(100))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Step 3: Training\n",
    "model.fit(X, y, epochs=10, verbose=1)\n",
    "\n",
    "# Step 4: Prediction Function\n",
    "def predict_next_word(model, tokenizer, text):\n",
    "    # Use custom tokenizer for the input text\n",
    "    token_list = custom_tokenize(text)\n",
    "    token_list = tokenizer.texts_to_sequences([' '.join(token_list)])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_length - 1, padding='pre')\n",
    "    predicted = model.predict(token_list, verbose=0)\n",
    "    predicted_word_index = np.argmax(predicted, axis=-1)\n",
    "    return tokenizer.index_word[predicted_word_index[0]]\n",
    "\n",
    "# Example usage\n",
    "input_text = 'int ys(int x,'\n",
    "predicted_word = predict_next_word(model, tokenizer, input_text)\n",
    "print(f'Next word prediction: {predicted_word}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pp8oF56297Ik",
    "outputId": "7b0462cc-1ce9-4ff8-fb10-ebcb8ebb6dd3"
   },
   "outputs": [],
   "source": [
    "input_text = 'int main void a int i'\n",
    "predicted_word = predict_next_word(model, tokenizer, input_text)\n",
    "print(f'Next word prediction: {predicted_word}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 227
    },
    "id": "fkgv4UYN4vxk",
    "outputId": "8b3c63c9-511f-494c-d2b7-9542c312544c"
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "id": "S5oK4KpVoBKw",
    "outputId": "348bd394-72b6-4e3d-9040-c1341690c8cc"
   },
   "outputs": [],
   "source": [
    "int('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MRPZqDhCPlIr",
    "outputId": "d2f00ed1-2ebe-4623-c899-8075757dc6eb"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LyL_8lK4lYNB",
    "outputId": "918ab016-e47a-4eec-9fab-a9cf5a7364b9"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Load the data from train.json\n",
    "def load_data(file_path, sample_fraction=0.01):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    # Sample 1% of the data\n",
    "    sample_size = max(1, int(len(data) * sample_fraction))\n",
    "    return [item['code'] for item in data[:sample_size]]\n",
    "\n",
    "# Custom tokenization function to include symbols and ignore comments\n",
    "def custom_tokenize(code):\n",
    "    code = re.sub(r'//.*', '', code)  # Remove everything after //\n",
    "    tokens = re.findall(r'\\w+|[^\\w\\s]', code)\n",
    "    return tokens\n",
    "\n",
    "# Prepare the dataset\n",
    "codes = load_data('train.jsonl')\n",
    "data = ' '.join(codes)\n",
    "data = 'nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnkkkknn'.join(codes)\n",
    "\n",
    "\n",
    "# Tokenization\n",
    "input_sequences = []\n",
    "for line in data.split('nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnkkkknn'):\n",
    "    token_list = custom_tokenize(line)\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i + 1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "# Create the tokenizer and fit on the tokenized sequences\n",
    "tokenizer = Tokenizer(filters='', lower=True, split=' ')\n",
    "tokenizer.fit_on_texts([' '.join(seq) for seq in input_sequences])\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "input_sequences = [tokenizer.texts_to_sequences([' '.join(seq)])[0] for seq in input_sequences]\n",
    "\n",
    "# Pad sequences\n",
    "max_sequence_length = max(len(x) for x in input_sequences)\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre')\n",
    "\n",
    "# Create predictors and labels\n",
    "X, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n",
    "\n",
    "# Step 2: Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Model Creation\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length=max_sequence_length - 1))\n",
    "model.add(SimpleRNN(100))\n",
    "model.add(Dropout(0.2))  # Add dropout layer\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks for early stopping and model checkpointing\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint('best_model.keras', monitor='val_accuracy', save_best_only=True)\n",
    "\n",
    "# Step 4: Training with validation accuracy\n",
    "history = model.fit(X_train, y_train, epochs=10, verbose=1, validation_data=(X_test, y_test),\n",
    "                    callbacks=[early_stopping, checkpoint])\n",
    "model.save('/content/drive/My Drive/my_model_autocomplete_rnn.keras')\n",
    "\n",
    "# Step 5: Prediction Function\n",
    "def predict_next_word(model, tokenizer, text):\n",
    "    token_list = custom_tokenize(text)\n",
    "    token_list = tokenizer.texts_to_sequences([' '.join(token_list)])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_length - 1, padding='pre')\n",
    "    predicted = model.predict(token_list, verbose=0)\n",
    "    predicted_word_index = np.argmax(predicted, axis=-1)\n",
    "    return tokenizer.index_word[predicted_word_index[0]]\n",
    "\n",
    "# Example usage\n",
    "input_text = 'int ys(int x,'\n",
    "predicted_word = predict_next_word(model, tokenizer, input_text)\n",
    "print(f'Next word prediction: {predicted_word}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iyEL0F9WjQ6l"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "EPhETBdvjOqf",
    "outputId": "ac1ea48b-514a-467d-aeac-a2c6a2217656"
   },
   "outputs": [],
   "source": [
    "plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NPPjgkD7h-6N",
    "outputId": "5ed1f79c-1562-4928-9842-e1d627f6a79e"
   },
   "outputs": [],
   "source": [
    "input_text = '''int ys(int x,int min){\n",
    "  int i,j;\n",
    "  if(a==1'''\n",
    "predicted_word = predict_next_word(model, tokenizer, input_text)\n",
    "print(f'Next word prediction: {predicted_word}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A2MuCjm7QIMl"
   },
   "outputs": [],
   "source": [
    "model.save('/content/drive/My Drive/my_model_autocomplete_rnn.keras')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tK8mPTDhjz9o"
   },
   "source": [
    "# LSTM CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bCv8hiVQj2KT",
    "outputId": "4425e352-ea5b-46d6-b231-4f744269c924"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout,LSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Load the data from train.json\n",
    "def load_data(file_path, sample_fraction=0.01):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    # Sample 1% of the data\n",
    "    sample_size = max(1, int(len(data) * sample_fraction))\n",
    "    return [item['code'] for item in data[:sample_size]]\n",
    "\n",
    "# Custom tokenization function to include symbols and ignore comments\n",
    "def custom_tokenize(code):\n",
    "    code = re.sub(r'//.*', '', code)  # Remove everything after //\n",
    "    tokens = re.findall(r'\\w+|[^\\w\\s]', code)\n",
    "    return tokens\n",
    "\n",
    "# Prepare the dataset\n",
    "codes = load_data('train.jsonl')\n",
    "data = ' '.join(codes)\n",
    "data = 'nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnkkkknn'.join(codes)\n",
    "\n",
    "\n",
    "# Tokenization\n",
    "input_sequences = []\n",
    "for line in data.split('nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnkkkknn'):\n",
    "    token_list = custom_tokenize(line)\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i + 1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "# Create the tokenizer and fit on the tokenized sequences\n",
    "tokenizer = Tokenizer(filters='', lower=True, split=' ')\n",
    "tokenizer.fit_on_texts([' '.join(seq) for seq in input_sequences])\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "input_sequences = [tokenizer.texts_to_sequences([' '.join(seq)])[0] for seq in input_sequences]\n",
    "\n",
    "# Pad sequences\n",
    "max_sequence_length = max(len(x) for x in input_sequences)\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre')\n",
    "\n",
    "# Create predictors and labels\n",
    "X, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n",
    "\n",
    "# Step 2: Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Model Creation\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length=max_sequence_length - 1))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))  # Add dropout layer\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks for early stopping and model checkpointing\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint('best_model_lstm.keras', monitor='val_accuracy', save_best_only=True)\n",
    "\n",
    "# Step 4: Training with validation accuracy\n",
    "history = model.fit(X_train, y_train, epochs=10, verbose=1, validation_data=(X_test, y_test),\n",
    "                    callbacks=[early_stopping, checkpoint])\n",
    "model.save('/content/drive/My Drive/my_model_autocomplete_lstm.keras')\n",
    "\n",
    "# Step 5: Prediction Function\n",
    "def predict_next_word(model, tokenizer, text):\n",
    "    token_list = custom_tokenize(text)\n",
    "    token_list = tokenizer.texts_to_sequences([' '.join(token_list)])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_length - 1, padding='pre')\n",
    "    predicted = model.predict(token_list, verbose=0)\n",
    "    predicted_word_index = np.argmax(predicted, axis=-1)\n",
    "    return tokenizer.index_word[predicted_word_index[0]]\n",
    "\n",
    "# Example usage\n",
    "input_text = 'int ys(int x,'\n",
    "predicted_word = predict_next_word(model, tokenizer, input_text)\n",
    "print(f'Next word prediction: {predicted_word}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "v6_Dm9iomEDz",
    "outputId": "d4e3398d-544c-4981-f8df-01e07e4101cb"
   },
   "outputs": [],
   "source": [
    "plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UTvgJEjVBRAx",
    "outputId": "311f4d54-240f-4de0-b6d1-3a789113ecd6"
   },
   "outputs": [],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X1nD5rGbBQ9g"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YvD4Rj5QBQ7D",
    "outputId": "4fd340c3-69eb-47a8-deea-661fd24357fb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Custom tokenization function to include symbols and ignore comments\n",
    "def custom_tokenize(code):\n",
    "    # Remove comments that start with //\n",
    "    code = re.sub(r'//.*', '', code)  # Remove everything after //\n",
    "    # Capture words and punctuation as tokens\n",
    "    tokens = re.findall(r'\\w+|[^\\w\\s]', code)\n",
    "    return tokens\n",
    "\n",
    "# Custom text for testing\n",
    "custom_text = \"\"\"\n",
    "// This is a sample function\n",
    "int add(int a, int b) {\n",
    "    return a + b; // Add two numbers\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Initialize the tokenizer with filters (but allow punctuation)\n",
    "tokenizer = Tokenizer(filters='', lower=True, split=' ')\n",
    "\n",
    "# Tokenize the custom text using the custom_tokenize function\n",
    "tokens = custom_tokenize(custom_text)\n",
    "\n",
    "# Fit the tokenizer on the tokens generated\n",
    "tokenizer.fit_on_texts([' '.join(tokens)])  # Use space-joined tokens for fitting\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Output the tokens and the tokenizer's word index\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Tokenizer Word Index:\", tokenizer.word_index)\n",
    "print(\"Total Words:\", total_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401
    },
    "id": "EJnIhtHFBsL2",
    "outputId": "23787bfd-86ac-4e00-da55-00117220123b"
   },
   "outputs": [],
   "source": [
    "input_sequences = [tokenizer.texts_to_sequences([' '.join(seq)])[0] for seq in input_sequences]\n",
    "\n",
    "# Pad sequences\n",
    "max_sequence_length = max(len(x) for x in input_sequences)\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre')\n",
    "\n",
    "# Create predictors and labels\n",
    "X, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n",
    "\n",
    "# Step 2: Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Model Creation\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length=max_sequence_length - 1))\n",
    "model.add(SimpleRNN(100))\n",
    "model.add(Dropout(0.2))  # Add dropout layer\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks for early stopping and model checkpointing\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint('best_model.keras', monitor='val_accuracy', save_best_only=True)\n",
    "\n",
    "# Step 4: Training with validation accuracy\n",
    "history = model.fit(X_train, y_train, epochs=10, verbose=1, validation_data=(X_test, y_test),\n",
    "                    callbacks=[early_stopping, checkpoint])\n",
    "\n",
    "# Step 5: Prediction Function\n",
    "def predict_next_word(model, tokenizer, text):\n",
    "    token_list = custom_tokenize(text)\n",
    "    token_list = tokenizer.texts_to_sequences([' '.join(token_list)])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_length - 1, padding='pre')\n",
    "    predicted = model.predict(token_list, verbose=0)\n",
    "    predicted_word_index = np.argmax(predicted, axis=-1)\n",
    "    return tokenizer.index_word[predicted_word_index[0]]\n",
    "\n",
    "# Example usage\n",
    "input_text = 'int ys(int x,'\n",
    "predicted_word = predict_next_word(model, tokenizer, input_text)\n",
    "print(f'Next word prediction: {predicted_word}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wzkEBbALBQ4N",
    "outputId": "0b8c0d2c-b6cb-4415-d044-4ab19f9dae2c"
   },
   "outputs": [],
   "source": [
    "[' '.join(seq) for seq in input_sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g3N9EoX5BD_M",
    "outputId": "7f8b549b-b029-458c-db52-468e894dae19"
   },
   "outputs": [],
   "source": [
    "input_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xjOd0tnrnA_n",
    "outputId": "d3a1155c-c30a-4398-db98-5a2a3da74603"
   },
   "outputs": [],
   "source": [
    "print(\"Tokenizer Word Index:\", tokenizer.word_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ndWiOQewA_vx",
    "outputId": "facc7121-ddcd-4ca1-f595-34a0b2293eb0"
   },
   "outputs": [],
   "source": [
    "tokenizer.word_index[\"int\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QJCDhZ5tloO8",
    "outputId": "d76f8374-3c4a-46db-c490-071ae3c839e4"
   },
   "outputs": [],
   "source": [
    "tokenizer.word_index['int']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VAlGciMAHCny",
    "outputId": "80d816d1-8c2f-4487-a37e-13c4d14bbf13"
   },
   "outputs": [],
   "source": [
    "input_text = 'int main int'\n",
    "predicted_word = predict_next_word(model, tokenizer, input_text)\n",
    "print(f'Next word prediction: {predicted_word}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zyxEh_R9M0x0",
    "outputId": "7760a391-236b-4f6e-8188-3f1fcebd589e"
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 227
    },
    "id": "0VlS_HUVM-7-",
    "outputId": "43f83343-6750-4104-d52e-cd48053ce439"
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FXMvq-vzjeMz"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def custom_tokenize(code):\n",
    "    # Regular expression to match words and symbols\n",
    "    tokens = re.findall(r'\\w+|[^\\w\\s]', code)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uM15ddifNLss",
    "outputId": "41b4ed21-f9cf-4aee-ff20-3bcde3822425"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Custom tokenization function to include symbols\n",
    "def custom_tokenize(code):\n",
    "    # Regular expression to match words and symbols\n",
    "    code = re.sub(r'//.*', '', code)  # Remove everything after //\n",
    "\n",
    "    tokens = re.findall(r'\\w+|[^\\w\\s]', code)\n",
    "    return tokens\n",
    "\n",
    "# Assuming `data` contains your loaded code snippets\n",
    "input_sequences = []\n",
    "for line in data.split('\\n'):\n",
    "    print(line)\n",
    "\n",
    "    # Use custom tokenizer instead of the default tokenizer\n",
    "    token_list = custom_tokenize(line)\n",
    "    print(\"Token List:\", token_list)  # Show the token list for debugging\n",
    "\n",
    "    u = []\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i + 1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "        u.append(n_gram_sequence)\n",
    "\n",
    "    print(\"UUU\", u)\n",
    "\n",
    "# Continue with your existing code to create sequences and train the model...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s_GpZH90iPIz",
    "outputId": "1be4c2da-c234-418b-a8fd-0dd4f13670aa"
   },
   "outputs": [],
   "source": [
    "n_gram_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MXVt8PJiiTdc",
    "outputId": "ff686742-d54b-4aa7-9304-ec818dada6fc"
   },
   "outputs": [],
   "source": [
    "input_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7jwO-lg6iggb",
    "outputId": "5f246447-0fcb-4407-c18c-fe194fa645b1"
   },
   "outputs": [],
   "source": [
    "# Assuming you have already fit the tokenizer on your data\n",
    "word_index = tokenizer.word_index  # This is the mapping of words to indices\n",
    "index_word = tokenizer.index_word   # This is the mapping of indices to words\n",
    "\n",
    "# To get the word corresponding to the index 19\n",
    "for i in range(100):\n",
    "  index_to_check = i\n",
    "  word = index_word.get(index_to_check)\n",
    "\n",
    "  if word:\n",
    "      print(f'The word corresponding to index {index_to_check} is: {word}')\n",
    "  else:\n",
    "      print(f'Index {index_to_check} does not correspond to any word.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X9ieV_YAip0c"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
